name: vllm-starter

x-common: &common
  # if you want to use a custom Dockerfile, uncomment the following lines and comment out the image line
  # build:
  #   context: .
  #   dockerfile: Dockerfile
  restart: unless-stopped
  image: yugasun/vllm-starter:latest
  container_name: vllm-starter
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"
  env_file:
    - .env
  environment:
    CUDA_VISIBLE_DEVICES: 0,1
    VLLM_USE_MODELSCOPE: True
    HF_ENDPOINT: https://hf-mirror.com
    TZ: "Asia/Shanghai"
  volumes:
    - ./models:/models:rw # Please modify this to the actual model directory.
    - ./config.yaml:/config.yaml:rw

services:
  vllm:
    <<: *common
    ports:
      - "9090:5000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
