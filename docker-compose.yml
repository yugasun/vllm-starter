name: vllm-starter

x-common: &common
  build:
    context: .
    dockerfile: Dockerfile
  restart: unless-stopped
  image: vllm:latest
  container_name: vllm-starter
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"
  ports:
    - "9090:5000"
  env_file:
    - .env
  environment:
    VLLM_USE_MODELSCOPE: ${VLLM_USE_MODELSCOPE}
    HF_ENDPOINT: ${HF_ENDPOINT}
    TZ: ${TZ}
    SERVED_MODEL_NAME: ${SERVED_MODEL_NAME}
    MODEL_PATH: ${MODEL_PATH}
    CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES}
    TENSOR_PARALLEL_SIZE: ${TENSOR_PARALLEL_SIZE}
    GPU_MEMORY_UTILIZATION: ${GPU_MEMORY_UTILIZATION}
  volumes:
    - /opt/models:/models:rw # Please modify this to the actual model directory.

services:
  vllm:
    <<: *common
    command:
      [
        "--model",
        "${MODEL_PATH}",
        "--host",
        "0.0.0.0",
        "--port",
        "5000",
        "--tensor-parallel-size",
        "${TENSOR_PARALLEL_SIZE}",
        "--gpu-memory-utilization",
        "${GPU_MEMORY_UTILIZATION}",
        "--trust-remote-code",
      ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
